{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "import json\n",
    "import uuid\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from sqlglot import parse_one, exp\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "global_dataframe = pd.DataFrame()\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------#\n",
    "# QUERY\n",
    "# --------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "file_name = \"Life - LFN Pipeline Summary\"\n",
    "# file_name = \"Life - Wire Pipeline Reconciliation\"\n",
    "\n",
    "input_path = f\"./input/{file_name}.sql\"\n",
    "output_path = f\"./OutputQueries/{file_name}/Converted_{file_name}.sql\"\n",
    "with open(input_path, \"r\") as sql_file:\n",
    "    sql_string = sql_file.read()\n",
    "\n",
    "def handle_exception(exception_type, message):\n",
    "    print(\n",
    "        f\"EXCEPTION: {exception_type.__name__}\")\n",
    "    print(message)\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "    sys.exit()\n",
    "    \n",
    "\n",
    "\n",
    "class MoreThanOneRowRetrieved(Exception):\n",
    "    pass\n",
    "\n",
    "class NoReplacementFound(Exception):\n",
    "    pass\n",
    "\n",
    "class NoReplacementFound2(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "#utils.py\n",
    "def create_or_overwrite_folder(folder_path):\n",
    "    try:\n",
    "        if os.path.exists(folder_path):\n",
    "            shutil.rmtree(folder_path)\n",
    "        os.makedirs(folder_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "folder_path = f\"C:/Users/vbohara/Desktop/Varun/Convtr/OutputQueries/{file_name}\"\n",
    "create_or_overwrite_folder(folder_path)\n",
    "\n",
    "#utils.py\n",
    "def append_to_global_dataframe(alias, source_table, source_column, replacement, comment, global_dataframe):\n",
    "    global_dataframe = pd.concat([global_dataframe, pd.DataFrame({ \n",
    "                                        'ALIAS': [alias], \n",
    "                                        'SOURCE TABLE': [source_table], \n",
    "                                        'SOURCE COLUMN': [source_column], \n",
    "                                        'REPLACEMENT': [replacement],\n",
    "                                        'COMMENT' : [comment]\n",
    "                                        })], axis=0, ignore_index=True)\n",
    "    return global_dataframe\n",
    "#utils.py\n",
    "def lowercase_listElements(input_list):\n",
    "    return [x.lower() for x in input_list]\n",
    "\n",
    "\n",
    "#utlis.py\n",
    "def list_to_dataframe(column_name, list_name):\n",
    "    return pd.DataFrame({column_name:[]}) if not list_name else pd.DataFrame({column_name: list_name})\n",
    "\n",
    "#utils.py\n",
    "def get_mapping_dataframe(file_path):\n",
    "    mapping_dataframe = pd.read_excel(\n",
    "        file_path, sheet_name='Table_Mapping')\n",
    "    mapping_dataframe = mapping_dataframe.fillna('TBD')\n",
    "    mapping_dataframe = mapping_dataframe.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    return mapping_dataframe\n",
    "\n",
    "#utils.py\n",
    "def get_information_schema_df(file_path):\n",
    "    information_schema_df = pd.read_excel(\n",
    "        file_path)\n",
    "    information_schema_df = information_schema_df.fillna('TBD')\n",
    "    information_schema_df = information_schema_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    return information_schema_df\n",
    "\n",
    "#utils.py\n",
    "def remove_similar_duplicates(input_list, threshold=95):\n",
    "    unique_items = []\n",
    "    for item in input_list:\n",
    "        is_unique = True\n",
    "        for unique_item in unique_items:\n",
    "            if fuzz.ratio(item, unique_item) >= threshold:\n",
    "                is_unique = False\n",
    "                break\n",
    "        if is_unique:\n",
    "            unique_items.append(item)\n",
    "    return unique_items\n",
    "\n",
    "#utils.py\n",
    "def get_best_match(word, word_list, threshold=95):\n",
    "    best_match = process.extractOne(word.upper(), word_list, scorer=fuzz.ratio)\n",
    "    return best_match[0] if best_match[1] >= threshold else None \n",
    "\n",
    "#utils.py\n",
    "def extract_from_and_join_clauses(text):\n",
    "    pattern = r'(?i)(from|join)\\s+(\\S+)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "#utils.py\n",
    "def extract_aliases_with_as_from_sql(query):\n",
    "    pattern = r'as\\s+(?:\"[^\"]*\"|\\[[^\\]]*\\]|\\w+|\\$+\\w+\\$+|\\s*\\'[^\\'\\n]*\\'\\s*)'\n",
    "    matches = re.findall(pattern, query, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    return matches\n",
    "\n",
    "#utils.py\n",
    "def extract_where_clauses(query):\n",
    "    # Split the query into clauses based on common SQL keywords\n",
    "    clauses = re.split(r'\\b(SELECT|FROM|WHERE|GROUP BY|JOIN)\\b', query, flags=re.IGNORECASE)\n",
    "    where_clauses = []\n",
    "    for i in range(1, len(clauses), 2):\n",
    "        clause_type = clauses[i].strip().lower()\n",
    "        if clause_type == 'where':\n",
    "            where_clauses.append(clauses[i + 1])\n",
    "    return where_clauses\n",
    "\n",
    "#utils.py\n",
    "def extract_and_or_of_where_clause(where_clause):\n",
    "    sections = re.split(r'\\b(?:and|or)\\b', where_clause)\n",
    "    sections = [section.strip() for section in sections]\n",
    "    return sections\n",
    "\n",
    "#utils.py\n",
    "def extract_on_clause_expressions(query):\n",
    "    # FROM THE EXTRACTED ON CLAUSES WE ARE IGNORING THOSE WITH CASE STATEMENTS\n",
    "    clauses = re.split(r'\\b(SELECT|FROM|WHERE|GROUP BY|ON|JOIN|CASE|WHEN|ELSE|END)\\b', query, flags=re.IGNORECASE)\n",
    "    on_clauses = []\n",
    "    for i in range(1, len(clauses), 2):\n",
    "        clause_type = clauses[i].strip().lower()\n",
    "        if clause_type == 'on':\n",
    "            on_clauses.append(clauses[i + 1])\n",
    "    # ONLY CHOSSING THOSE THAT HAVE = IN THEM \n",
    "    return [clause.strip() for clause in on_clauses if '=' in clause]\n",
    "\n",
    "#utils.py\n",
    "def preety_print_dict(dictionary):\n",
    "    dictionary = json.dumps(dictionary, indent=4)\n",
    "    print(dictionary)\n",
    "\n",
    "#utils.py     \n",
    "def tempoary_replace_alias_in_output_query(input_query, output_query):\n",
    "    #--------------------------------------------------------------------------------------------------------------------------#\n",
    "    alias_to_code_list = extract_aliases_with_as_from_sql(input_query)\n",
    "    #  Stripping leading and trailing whitespace, and removing any non-word and non-whitespace characters from each string.\n",
    "    for i in range(len(alias_to_code_list)):\n",
    "        alias_to_code_list[i] = re.sub(r'[^\\w\\s]', '', alias_to_code_list[i].strip())\n",
    "\n",
    "    #Generation of the code\n",
    "    alias_to_code = {item: f\" {str(uuid.uuid4())[:8]} \" for item in alias_to_code_list}\n",
    "    code_to_alias = {code: item for item, code in alias_to_code.items()}\n",
    "    #--------------------------------------------------------------------------------------------------------------------------#\n",
    "    #                                   OP. TEMPOARY REPLACE ALIAS IN OUTPUT QUERY \n",
    "    #--------------------------------------------------------------------------------------------------------------------------#\n",
    "    for key in alias_to_code:\n",
    "        output_query = output_query.replace(key, alias_to_code[key])\n",
    "                                                                #END OF ALIAS\n",
    "    #--------------------------------------------------------------------------------------------------------------------------#\n",
    "    return output_query, code_to_alias\n",
    "\n",
    "#utlis.py\n",
    "def dict_to_dataframe(input_dict,column1,column2):\n",
    "    data = {column1: list(input_dict.keys()), column2: [', '.join(value) for value in input_dict.values()]}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "#utils.py\n",
    "def single_dict_to_dataframe(input_dict,column1,column2):\n",
    "    data = {column1 : list(input_dict.keys()), column2: list(input_dict.values())}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "#table_utils.py\n",
    "def table_mapping(df,tables_in_sttm):\n",
    "    mapping_dict = {}\n",
    "    df = df[df['DW Table'].isin(tables_in_sttm)]\n",
    "    for index, row in df.iterrows():\n",
    "        source_table = row[\"DW Table\"].lower()\n",
    "        target_table = row[\"Physical Table Name\"]\n",
    "        mapping_dict[source_table] = target_table\n",
    "    return mapping_dict\n",
    "\n",
    "#table_utils.py\n",
    "def replace_table_name(query, table_name):\n",
    "    pattern = r'(\\w+)\\.' + re.escape(table_name) + r'\\b'\n",
    "    modified_query = re.sub(pattern, r'table_name', query)\n",
    "    return modified_query\n",
    "\n",
    "#tables_utils.py\n",
    "def generate_cte_statements_for_tables(df, table_list,data_type_defaults):\n",
    "    tables_not_to_be_considered_for_metadata = []\n",
    "    if not table_list:\n",
    "        return ''\n",
    "    cte_statements = {}\n",
    "    for table_name in table_list:\n",
    "        table_df = df[df['TABLE_NAME'] == table_name]\n",
    "        if table_df.empty:\n",
    "            print(f\"CTE NOT GENERATED FOR {table_name}\")\n",
    "            # table_list.remove(table_name)\n",
    "            tables_not_to_be_considered_for_metadata.append(table_name)\n",
    "            if not table_list:\n",
    "                return ''\n",
    "            continue\n",
    "        cte = f\"{table_name} as (\\n\"\n",
    "        select_clause = f\"select \"\n",
    "        for index, row in table_df.iterrows():\n",
    "            column_name = row['COLUMN_NAME']\n",
    "            data_type = row['DATA_TYPE'].lower()\n",
    "            default_value = data_type_defaults.get(data_type)\n",
    "            select_clause += f\"{default_value} as {column_name}, \"\n",
    "        \n",
    "        select_clause = select_clause.rstrip(', ') \n",
    "        cte += select_clause + \"\\n),\\n\"\n",
    "        cte_statements[table_name] = cte\n",
    "    \n",
    "    cte_query = \"with \"+\"\\n\".join(cte_statements.values())\n",
    "    return cte_query, tables_not_to_be_considered_for_metadata\n",
    "\n",
    "#table_utils.py\n",
    "def find_table_aliases(query, table_names):\n",
    "    # Given a list of tables return a list of tuple (table_name,alias)\n",
    "    pattern = r'(\\b{}\\b)\\s+AS\\s+(\\w+)'.format('|'.join(table_names))\n",
    "    matches = re.findall(pattern, query, re.IGNORECASE)\n",
    "    matches = [(item[0].lower(), item[1]) for item in matches]\n",
    "    return matches\n",
    "\n",
    "#table_utils.py\n",
    "def create_alias_table_dict(table_alias_pairs):\n",
    "    # Given a list of tuple from above function -> {alias:[List of tables with that alias]}\n",
    "    alias_table_dict = {}\n",
    "    for table, alias in table_alias_pairs:\n",
    "        if alias not in alias_table_dict:\n",
    "            alias_table_dict[alias] = {table}\n",
    "        else:\n",
    "            alias_table_dict[alias].add(table)\n",
    "    return alias_table_dict\n",
    "    \n",
    "data_type_defaults ={\n",
    "                        'bigint' : 1,   'char':\"''\",\n",
    "                        'date':f\"date('01-01-1900')\",\n",
    "                        'datetime':\"date('01-01-1900')\",\n",
    "                        'datetime2':\"date('01-01-1900')\",\n",
    "                        'decimal':1.0,  'float':1.0,\n",
    "                        'int':1,'money':1,'numeric':1,'nvarchar':\"''\",'smallint':1,\n",
    "                        'tinyint':1,'varbinary':1,'varchar':\"''\"\n",
    "                    }\n",
    "\n",
    "#table_utils.py\n",
    "# TABLES IN QUERY = TABLES IN STTM(REPRESENTED WITH THE TABLE NAME FROM STTM) + TABLES NOT IN STTM(REPRESENTED AS NONE)\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "def extract_table_from_query(input_query, mapping_dataframe):\n",
    "    tables_in_query_tuple = extract_from_and_join_clauses(input_query)\n",
    "    tables_in_query_raw = [item[1] for item in tables_in_query_tuple]\n",
    "    tables_in_query_raw = [item for item in tables_in_query_raw if 'select' not in item.lower()]\n",
    "    unique_tables_in_query_raw = remove_similar_duplicates(tables_in_query_raw)\n",
    "    unique_tables_in_query_raw = [item.split('.')[-1] if '.' in item else item for item in unique_tables_in_query_raw]\n",
    "    translation_table = str.maketrans('', '', '()\";')\n",
    "    unique_tables_in_query_raw = [item.translate(translation_table) for item in unique_tables_in_query_raw]\n",
    "    sttm_tables = mapping_dataframe['DW Table'].unique().tolist()\n",
    "    sttm_tables = [item for item in sttm_tables if item!='TBD']\n",
    "    # TABLES IN STTM  = TABLES IN STTM WITH TARGET TABLES  + TABLES IN STTM WITHOUT TARGET TABLES \n",
    "    #--------------------------------------------------------------------------------------------------------------------------#\n",
    "    tables_in_query = [get_best_match(item, sttm_tables, threshold=95)\n",
    "                    for item in unique_tables_in_query_raw]\n",
    "    original_to_bestMatch = dict()\n",
    "    for original, best_match in zip(unique_tables_in_query_raw, tables_in_query):\n",
    "        original_to_bestMatch[original] = best_match\n",
    "    tables_not_in_sttm = [key for key in original_to_bestMatch.keys() if original_to_bestMatch[key] is  None]\n",
    "    tables_in_sttm = [value.lower() for value in original_to_bestMatch.values() if value is not None]\n",
    "    return tables_in_sttm,tables_not_in_sttm\n",
    "  \n",
    "#column_utils.py\n",
    "def extract_columns(tsql_query):\n",
    "    parsed_query = parse_one(tsql_query)\n",
    "    column_references = parsed_query.find_all(exp.Column)\n",
    "    column_names = [column.name for column in column_references]\n",
    "    return list(set(column_names))\n",
    "    \n",
    "#column_utils.py\n",
    "def column_mapping(df, information_schema_df, tables_in_sttm, tsql_query):\n",
    "    \"\"\"\n",
    "    Returns \n",
    "           Root Column names that are present in the query \n",
    "           Columns that do not belong to any tables that are present in the STTM\n",
    "    \"\"\"\n",
    "\n",
    "    #Extracting only those columns that are present in the STTM\n",
    "    df = df[df['DW Table'].isin(tables_in_sttm)]\n",
    "    information_schema_df = information_schema_df[information_schema_df['TABLE_NAME'].isin(tables_in_sttm)]\n",
    "    columns_in_query = []\n",
    "    columns_in_query = extract_columns(tsql_query)\n",
    "    #Making sure that all the columns for the tables that are mentioned in the query , are fetched\n",
    "    # for column in information_schema_df['COLUMN_NAME']: #Only those columns whose tables MIGHT BE mentioned in the query \n",
    "    #     pattern = r'\\b' + re.escape(column) + r'\\b'\n",
    "    #     if re.search(pattern, tsql_query): #and (column not in columns_in_query):\n",
    "    #         columns_in_query.append(column)\n",
    "    #Columns in sttm whose table is present in sttm\n",
    "    columns_in_sttm = df['DW Column'].tolist()\n",
    "    #Columns in query but not in STTM \n",
    "    columns_not_in_sttm = list(set(columns_in_query) - set(columns_in_sttm))\n",
    "    col_datatype_info_schma = information_schema_df[information_schema_df['COLUMN_NAME'].isin(columns_not_in_sttm)]\n",
    "    col_datatype_info_schma = col_datatype_info_schma.drop_duplicates(subset=[\"COLUMN_NAME\", \"DATA_TYPE\"])\n",
    "    columns_not_in_sttm = dict(zip(col_datatype_info_schma[\"COLUMN_NAME\"], col_datatype_info_schma[\"DATA_TYPE\"]))\n",
    "    return list(set(lowercase_listElements(columns_in_query))) , columns_not_in_sttm\n",
    "\n",
    "\n",
    "#column_utils.py\n",
    "def get_replcmnt_unique_child_instances(mapping_dataframe, \n",
    "                                        alias_table_dict, \n",
    "                                        child_instance_alias, \n",
    "                                        child_instance_root,\n",
    "                                        data_type_defaults,\n",
    "                                        information_schema_df):\n",
    "    try:\n",
    "        global global_dataframe\n",
    "        #Unique Alias-> Has only 1 Table in it's set \n",
    "        # THIS CONDITION SHOULD RETURN ONLY 1 ROW\n",
    "        filtered_df = mapping_dataframe[\n",
    "            (mapping_dataframe['DW Table'] == (list(alias_table_dict[child_instance_alias])[0].lower())) &\n",
    "            (mapping_dataframe['DW Column'] == child_instance_root.lower()) &\n",
    "            (mapping_dataframe['Physical Column Name'] !='tbd')\n",
    "            ]\n",
    "        if not filtered_df.empty:\n",
    "            if filtered_df.shape == (1, filtered_df.shape[1]):\n",
    "                global_dataframe = append_to_global_dataframe(child_instance_alias.lower(),\n",
    "                                                              ', '.join(\n",
    "                                                                  lowercase_listElements(filtered_df['DW Table'].tolist())),\n",
    "                                                              ', '.join(\n",
    "                                                                  lowercase_listElements(filtered_df['DW Column'].tolist())),\n",
    "                                                            filtered_df['Physical Column Name'].iloc[0], \n",
    "                                                            \"Unique Alias : Replacement found in STTM\",\n",
    "                                                            global_dataframe)\n",
    "                return filtered_df['Physical Column Name'].iloc[0]\n",
    "            else: \n",
    "                raise MoreThanOneRowRetrieved(\"MORE THAN 1 ROW RETRIEVED\")\n",
    "        else:\n",
    "            # This case infers that source/target column is missing (TBD) (THE 2nd or 3rd CONDITION FAILED)\n",
    "            # Because the values of Table and column will always be present\n",
    "            global_dataframe = append_to_global_dataframe(child_instance_alias.lower(),\n",
    "                                                            list(alias_table_dict[child_instance_alias])[0].lower(), \n",
    "                                                            child_instance_root.lower(), \n",
    "                                                            f\"/*{child_instance_root} */\", \n",
    "                                                              f\"Commented.\",\n",
    "                                                            global_dataframe)\n",
    "            return f\"/*{child_instance_root} */\"\n",
    "        \n",
    "    \n",
    "    except MoreThanOneRowRetrieved as e:\n",
    "        return handle_exception(MoreThanOneRowRetrieved,f\"CHECK STTM\\n{child_instance_alias} : {list(alias_table_dict[child_instance_alias])[0]} : {child_instance_root}\\n{filtered_df}\")\n",
    "    except NoReplacementFound as e:\n",
    "        return handle_exception(NoReplacementFound,f\"THIS CASE SHOULD NEVER OCCUR, SOMETHING WRONG WITH THE INFORMATION SCHEMA\\n{child_instance_alias} : {list(alias_table_dict[child_instance_alias])[0]} : {child_instance_root}\")\n",
    "    except Exception as e:\n",
    "        return handle_exception(Exception,str(e))\n",
    "    \n",
    "\n",
    "#column_utils.py\n",
    "def get_child_instances(root_col,query):\n",
    "    \"\"\"\n",
    "    INPUT : GIVEN A ROOT COLUMN AND THE QUERY, \n",
    "    OUTPUT: A LIST OF ALL THE INSTANCES OF THE ROOT COLUMN \n",
    "            METADATA ON HOW MAY STAND ALONE INSTNACES \n",
    "    \"\"\"\n",
    "    pattern = r'\\b\\w*\\.' + re.escape(root_col) + r'\\b|\\b' + re.escape(root_col) + r'\\b'\n",
    "    matches = re.findall(pattern, query, re.IGNORECASE)\n",
    "    set_column_instances = list(set(matches))\n",
    "    #Focus on the columns with '.'\n",
    "    column_instance_list = [item for item in set_column_instances if '.' in item]\n",
    "    number_of_standalone_root_col = len([item for item in set_column_instances if '.' not in item])\n",
    "    root_col_meta_data ={'number_of_standalone_root_col':number_of_standalone_root_col}\n",
    "    return list(set(lowercase_listElements(column_instance_list))), root_col_meta_data\n",
    "\n",
    "#column_utils.py\n",
    "def get_replcmnt_duplicate_child_instances(mapping_dataframe,\n",
    "                                            alias_table_dict,\n",
    "                                              child_instance_alias,\n",
    "                                                child_instance_root,\n",
    "                                                data_type_defaults,\n",
    "                                                information_schema_df):\n",
    "    try:\n",
    "        global global_dataframe\n",
    "        # Filter the DataFrame based on conditions\n",
    "        filtered_df = mapping_dataframe[\n",
    "            (mapping_dataframe['DW Table'].isin(lowercase_listElements(alias_table_dict[child_instance_alias]))) &\n",
    "        (mapping_dataframe['DW Column'] == child_instance_root.lower()) &\n",
    "        (mapping_dataframe['Physical Column Name'] != 'tbd')\n",
    "        ]\n",
    "        if not filtered_df.empty:\n",
    "            global_dataframe = append_to_global_dataframe(child_instance_alias.lower(),\n",
    "                                                          ', '.join(lowercase_listElements(\n",
    "                                                              alias_table_dict[child_instance_alias])),\n",
    "                                                            child_instance_root.lower(),\n",
    "                                                          ', '.join(lowercase_listElements(list(\n",
    "                                                              filtered_df['Physical Column Name']))),\n",
    "                                                            f\"Duplicate alias : Found replacement in STTM\",\n",
    "                                                            global_dataframe)\n",
    "            return list(filtered_df['Physical Column Name'])\n",
    "        else:           \n",
    "            global_dataframe = append_to_global_dataframe(child_instance_alias,\n",
    "                                                            ', '.join(alias_table_dict[child_instance_alias]), \n",
    "                                                            child_instance_root, \n",
    "                                                            f\"/*{child_instance_root}*/\", \n",
    "                                                            f\"Commented..\",\n",
    "                                                            global_dataframe)\n",
    "            return f\"/*{child_instance_root}/*\"\n",
    "    except NoReplacementFound2 as e:\n",
    "        return handle_exception(NoReplacementFound2,\n",
    "                                f\"\"\"EXCEPTION in get_replcmnt_duplicate_child_instances()\n",
    "                                THIS CASE SHOULD NEVER OCCUR, SOMETHING WRONG WITH THE INFORMATION SCHEMA\n",
    "                                {child_instance_alias} : {alias_table_dict[child_instance_alias]} : {child_instance_root}\"\"\")\n",
    "\n",
    "\n",
    "#column_utils.py\n",
    "#IT SHOULD BE COLUMNS_IN_QUERY AND NOT COLUMNS_IN_STTM\n",
    "def extract_all_column_instance_replacements(columns_in_sttm,query,alias_table_dict,duplicate_alias_to_tables,mapping_dataframe,information_schema_df):\n",
    "    global file_name\n",
    "    # Given a set of all the root column names mentioned in the query \n",
    "    # Return a dictionary having (child column : replacements)\n",
    "    metadata_child_instances_to_replacements = dict()\n",
    "    child_instances_to_replace = dict()\n",
    "\n",
    "    root_columns_in_sttm = columns_in_sttm\n",
    "    pprint(list_to_dataframe('ROOT COLUMNS IN STTM',root_columns_in_sttm))\n",
    "    standalone_root_columns = {}\n",
    "    for root_column in root_columns_in_sttm:\n",
    "        # GETTING THE CHILD INSTANCES OF THE ROOT COLUMN OF CURRENT ITERATION\n",
    "        child_instances, root_col_meta_data = get_child_instances(root_column,query)\n",
    "        if not child_instances:\n",
    "            #These columns might have come from information_schema but haven't been used in the query \n",
    "            if root_col_meta_data['number_of_standalone_root_col'] == 0 :\n",
    "                continue\n",
    "            else:\n",
    "                # This means that the root column is not mentioned in the query using `.root_column` , it's directly mentioned `root_column`\n",
    "                # CONCATENATE RHS WITH IDENTIFIER TO CHECK WHICH COLUMNS ARE FROM THIS LOC\n",
    "                standalone_root_columns[root_column] = str(root_col_meta_data['number_of_standalone_root_col'])\n",
    "        else:\n",
    "            metadata_child_instances_to_replacements[root_column] = {}\n",
    "            # standalone_root_columns[root_column] = str(root_col_meta_data['number_of_standalone_root_col']) # UNCOMMENT IF WANT TO CHECK 0 VALUES \n",
    "            if root_col_meta_data['number_of_standalone_root_col'] !=0 :\n",
    "                standalone_root_columns[root_column] = str(root_col_meta_data['number_of_standalone_root_col'])  \n",
    "            for child_instance in child_instances:\n",
    "                child_instance_alias,child_instance_root = child_instance.split('.')[0],child_instance.split('.')[-1]\n",
    "                #Child instance with only one possible replacement , alias is not duplicated\n",
    "                if child_instance_alias not in duplicate_alias_to_tables.keys():\n",
    "                    # print(child_instance_alias)\n",
    "                    # Ignoring those aliases that belong to tables_not_in_sttm, alias_table_dict has only those tables that are present in STTM\n",
    "                    if child_instance_alias in alias_table_dict.keys():\n",
    "                        replacement =  get_replcmnt_unique_child_instances(mapping_dataframe, \n",
    "                                                                            alias_table_dict, \n",
    "                                                                            child_instance_alias, \n",
    "                                                                            child_instance_root,\n",
    "                                                                            data_type_defaults,\n",
    "                                                                            information_schema_df)\n",
    "                        if '/*' in (replacement): # SOURCE/TARGET COLUMN MISSING \n",
    "                            metadata_child_instances_to_replacements[root_column][child_instance]=[replacement]\n",
    "                            child_instances_to_replace[child_instance] = replacement\n",
    "                        else:\n",
    "                            metadata_child_instances_to_replacements[root_column][child_instance]=[child_instance.replace(child_instance_root,replacement)]\n",
    "                            child_instances_to_replace[child_instance] = child_instance.replace(child_instance_root,replacement)\n",
    "                            \n",
    "                    else:\n",
    "                        global global_dataframe\n",
    "                        #This alias is of that table which is not present in STTM , and there by has a CTE created for it \n",
    "                        #This tell us that we are not replacing those columns as we are creating a CTE\n",
    "                        metadata_child_instances_to_replacements[root_column][child_instance]=[child_instance] #KEEPING IT AS IT IS\n",
    "                        child_instances_to_replace[child_instance] = child_instance\n",
    "                        global_dataframe = append_to_global_dataframe(child_instance_alias,\n",
    "                                                          child_instance_alias, \n",
    "                                                          child_instance_root, \n",
    "                                                          child_instance, \n",
    "                                                          f\"CTE: {child_instance_alias} is an alias of table that is not present in the STTM\",\n",
    "                                                          global_dataframe)\n",
    "\n",
    "                #Child instance with more than one possible replacement , alias is duplicated\n",
    "                else:\n",
    "                    replacement_list = get_replcmnt_duplicate_child_instances(mapping_dataframe, \n",
    "                                                                            alias_table_dict, \n",
    "                                                                            child_instance_alias, \n",
    "                                                                            child_instance_root,\n",
    "                                                                            data_type_defaults,\n",
    "                                                                            information_schema_df\n",
    "                                                                            )\n",
    "                    if isinstance(replacement_list,str): # SOURCE/TARGET COLUMN MISSING \n",
    "                        metadata_child_instances_to_replacements[root_column][child_instance]= replacement_list\n",
    "                        child_instances_to_replace[child_instance] = replacement_list \n",
    "                    else:\n",
    "                        replacement_list = [ child_instance.replace(child_instance_root,item) for item in replacement_list]\n",
    "                        metadata_child_instances_to_replacements[root_column][child_instance]= replacement_list\n",
    "                        child_instances_to_replace[child_instance] = str(replacement_list[0]) \n",
    "    with open(\"C:/Users/vbohara/Desktop/Varun/Convtr/metadata_child_instances_to_replacements.json\", 'w') as json_file:\n",
    "        json.dump(metadata_child_instances_to_replacements, json_file, indent=4)\n",
    "    pprint(dict_to_dataframe(standalone_root_columns,\"ROOT COLUMN\" , \"STANDALONE ROOT COLUMNS\"))\n",
    "    global_dataframe = global_dataframe.sort_values(by=['ALIAS', 'SOURCE COLUMN'])\n",
    "    global_dataframe.to_excel(f\"./OutputQueries/{file_name}/Mapping_{file_name}.xlsx\", index=False)\n",
    "    return child_instances_to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP_1 : Replace the extra spaces after AS\n",
      "STEP_2 : EXTRACT TABLES IN STTM AND TABLES NOT IN STTM\n",
      "\n",
      "TABLES IN STTM\n",
      "               TABLES IN STTM\n",
      "0  rpt_dim_is_policy_pend_ext\n",
      "1        fct_daily_trans_smry\n",
      "2             rpt_dim_channel\n",
      "3             rpt_dim_product\n",
      "4     ref_is_pend_status_code\n",
      "5              rpt_dim_policy\n",
      "6                rpt_dim_firm\n",
      "7             rpt_dim_advisor\n",
      "8              rpt_dim_agency\n",
      "\n",
      "TABLES NOT IN STTM\n",
      "                        TABLES NOT IN STTM\n",
      "0                rpt_dim_advisor_hierarchy\n",
      "1  #t2_life_lfn_pipeline_summary_procedure\n",
      "\n",
      "STEP_3 : EXTRACT ALIAS TO TABLE MAPPING AND IDENTIFY DUPLICATE ALIAS IF ANY\n",
      "Alias to Table Mapping\n",
      "\n",
      "  ALIAS                 TABLE NAMES\n",
      "0   ppe  rpt_dim_is_policy_pend_ext\n",
      "1     f        fct_daily_trans_smry\n",
      "2   rdc             rpt_dim_channel\n",
      "3    pr             rpt_dim_product\n",
      "4   psc     ref_is_pend_status_code\n",
      "5   rdp              rpt_dim_policy\n",
      "6    df                rpt_dim_firm\n",
      "7   rsm             rpt_dim_advisor\n",
      "8   rda              rpt_dim_agency\n",
      "\n",
      "NO DUPLICATE ALIAS FOUND IN THE QUERY!!\n",
      "\n",
      "STEP_4 : GET THE TABLE MAPPING\n",
      "                 SOURCE TABLE                            TARGET TABLE\n",
      "0                rpt_dim_firm                                dim_firm\n",
      "1             rpt_dim_channel  bridge channel subchannel relationship\n",
      "2             rpt_dim_advisor                               dim_advsr\n",
      "3              rpt_dim_agency                                dim_agcy\n",
      "4             rpt_dim_product                                dim_prod\n",
      "5        fct_daily_trans_smry                        fct_dly_plcy_trx\n",
      "6              rpt_dim_policy                                dim_plcy\n",
      "7  rpt_dim_is_policy_pend_ext                                dim_plcy\n",
      "8     ref_is_pend_status_code                                dim_plcy\n",
      "\n",
      "STEP_5 : TEMPOARY REPLACE ALIAS IN OUTPUT QUERY\n",
      "        ROOT COLUMNS IN STTM\n",
      "0               prod_premium\n",
      "1             sub_channel_cd\n",
      "2                 lob_key_cd\n",
      "3                 advisor_id\n",
      "4            app_type_key_cd\n",
      "5                    rsm_key\n",
      "6               lob_key_desc\n",
      "7           status_cd_key_id\n",
      "8                 product_cd\n",
      "9                firm_cdw_id\n",
      "10            target_premium\n",
      "11            channel_key_id\n",
      "12            product_key_id\n",
      "13  advisor_hierarchy_key_id\n",
      "14         product_type_desc\n",
      "15            status_cd_desc\n",
      "16             agency_cdw_id\n",
      "17             policy_key_id\n",
      "18             agency_key_id\n",
      "19             policy_number\n",
      "20      transaction_group_cd\n",
      "21     rop_threshold_premium\n",
      "22      product_sub_group_cd\n",
      "23        is_compensible_flg\n",
      "24          total_policy_val\n",
      "25                 full_name\n",
      "26           product_type_cd\n",
      "27                channel_cd\n",
      "28               firm_key_id\n",
      "29               status_type\n",
      "Empty DataFrame\n",
      "Columns: [ROOT COLUMN, STANDALONE ROOT COLUMNS]\n",
      "Index: []\n",
      "CTE NOT GENERATED FOR #t2_life_lfn_pipeline_summary_procedure\n"
     ]
    }
   ],
   "source": [
    "input_query = sql_string.lower() # DID THIS TO RESOLVE THE ISSUE OF CASE SENSITIVITY, ESPECIALLY IN UPPER CASE ALIAS \n",
    "input_query_where_clauses_list = extract_where_clauses(input_query)\n",
    "input_query_on_clause_list = extract_on_clause_expressions(input_query)\n",
    "output_query = input_query\n",
    "\n",
    "#STEP_1 : Replace the extra spaces after AS\n",
    "print(\"STEP_1 : Replace the extra spaces after AS\")\n",
    "input_query = re.sub(r'\\bas\\s+', 'as ', input_query)\n",
    "output_query = re.sub(r'\\bas\\s+', 'as ', output_query)\n",
    "\n",
    "#--------------------------------------------------------DATAFRAME---------------------------------------------------------# \n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "file_path = \"C:/Users/vbohara/Desktop/Varun/Convtr/dataset/sttm_new.xlsx\"\n",
    "mapping_dataframe = get_mapping_dataframe(file_path)\n",
    "file_path = \"C:/Users/vbohara/Desktop/Varun/Convtr/dataset/INFORMATION_SCHEMA.xlsx\"\n",
    "information_schema_df = get_information_schema_df(file_path)\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#STEP_2 : Extract tables_in_sttm and tables_not_in_sttm\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "tables_in_sttm,tables_not_in_sttm = extract_table_from_query(input_query, mapping_dataframe)\n",
    "print(\"STEP_2 : EXTRACT TABLES IN STTM AND TABLES NOT IN STTM\\n\")\n",
    "print(\"TABLES IN STTM\")\n",
    "pprint(list_to_dataframe(\"TABLES IN STTM\",tables_in_sttm))\n",
    "print(\"\\nTABLES NOT IN STTM\")\n",
    "pprint(list_to_dataframe(\"TABLES NOT IN STTM\",tables_not_in_sttm))\n",
    "\n",
    "#STEP_3 : Extract Alias to Table Mapping and Identify duplicate alias if any \n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "tables_in_query = tables_in_sttm #TBD + tables_not_in_sttm\n",
    "table_aliases = find_table_aliases(input_query, tables_in_query)\n",
    "alias_table_dict = create_alias_table_dict(table_aliases)\n",
    "duplicate_alias_to_tables = dict()\n",
    "print(\"\\nSTEP_3 : Extract Alias to Table Mapping and Identify duplicate alias if any\".upper())\n",
    "print(f\"Alias to Table Mapping\\n\")\n",
    "pprint(dict_to_dataframe(alias_table_dict,\"ALIAS\",\"TABLE NAMES\"))\n",
    "for alias, tables in alias_table_dict.items():\n",
    "    if len(tables)>1:\n",
    "        duplicate_alias_to_tables[alias] = tables\n",
    "\n",
    "if list(duplicate_alias_to_tables.keys()):\n",
    "    print(f\"\\nDuplicate Alias in the queries\\t{list(duplicate_alias_to_tables.keys())}\\n\")\n",
    "    pprint(dict_to_dataframe(duplicate_alias_to_tables,\"ALIAS\",\"TABLE NAMES\"))\n",
    "else:\n",
    "    print(\"\\nNO DUPLICATE ALIAS FOUND IN THE QUERY!!\") \n",
    "\n",
    "\n",
    "#STEP_4 : GET THE TABLE MAPPING\n",
    "table_mapping_dict = table_mapping(mapping_dataframe,tables_in_sttm)\n",
    "print(\"\\nSTEP_4 : GET THE TABLE MAPPING\")\n",
    "pprint(single_dict_to_dataframe(table_mapping_dict,\"SOURCE TABLE\",\"TARGET TABLE\"))\n",
    "columns_in_sttm, columns_not_in_sttm = column_mapping(mapping_dataframe,\n",
    "                                                      information_schema_df,\n",
    "                                                      tables_in_sttm,\n",
    "                                                      input_query)\n",
    "\n",
    "#STEP_5 : TEMPOARY REPLACE ALIAS IN OUTPUT QUERY\n",
    "print(\"\\nSTEP_5 : TEMPOARY REPLACE ALIAS IN OUTPUT QUERY\")\n",
    "output_query, code_to_alias = tempoary_replace_alias_in_output_query(input_query, output_query)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "#STEP_6 : GET REPLACEMENTS FOR COLUMNS  \n",
    "child_instances_to_replace =  extract_all_column_instance_replacements(columns_in_sttm,\n",
    "                                                                            output_query,\n",
    "                                                                            alias_table_dict,\n",
    "                                                                            duplicate_alias_to_tables,\n",
    "                                                                            mapping_dataframe,\n",
    "                                                                            information_schema_df)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "#STEP_7 : REPLACE TABLES\n",
    "table_dict = dict()\n",
    "for key in table_mapping_dict.keys():\n",
    "    if key in output_query:\n",
    "        #EDGECASE: TABLES IN STTM WITHOUT TARGET TABLES \n",
    "        if \"tbd\" in table_mapping_dict[key]:\n",
    "            # Send it for \"with clause\" operation\n",
    "            tables_not_in_sttm.append(key)\n",
    "            continue            \n",
    "        table_dict[key] = f\"{table_mapping_dict[key]}\"\n",
    "\n",
    "        # Replacing the table names\n",
    "        output_query = output_query.replace(key,f\"{table_mapping_dict[key]}\")\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "#STEP_8 : REPLACE COLUMNS IN STTM\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "sorted_keys = sorted(child_instances_to_replace.keys(), key=len, reverse=True)\n",
    "for key in sorted_keys:            \n",
    "    pattern = r'\\b' + re.escape(key) + r'\\b'\n",
    "    output_query = re.sub(pattern, f\"{child_instances_to_replace[key]}\", output_query, flags=re.IGNORECASE)\n",
    "#--------------------------------------------------------------------------------------------------------------------------#\n",
    "            \n",
    "cte_statements, tables_not_to_be_considered_for_metadata =  generate_cte_statements_for_tables(information_schema_df,\n",
    "                                                                                               tables_not_in_sttm,\n",
    "                                                                                               data_type_defaults)\n",
    "\n",
    "header_meta_data = f\"\"\"\n",
    "Script Generated By : Converter.py\n",
    "Date                : {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "Description         : \"Converted input.sql to redshift_output.sql\"\n",
    "\"\"\"\n",
    "\n",
    "for key in code_to_alias:\n",
    "    if key in output_query:\n",
    "        output_query = output_query.replace(key, code_to_alias[key])\n",
    "\n",
    "\n",
    "if 'with' in output_query and 'with' in cte_statements:\n",
    "    output_query = output_query.replace('with', '')\n",
    "    if cte_statements != '':\n",
    "        cte_statements = cte_statements +','\n",
    "\n",
    "output_content = (\n",
    "    f\"/*{header_meta_data}*/\\n\"\n",
    "    f\"{cte_statements}\\n\\n\"\n",
    "    f\"{output_query}\\n\"\n",
    ")\n",
    "\n",
    "with open(f\"C:/Users/vbohara/Desktop/Varun/Convtr/OutputQueries/{file_name}/{file_name}_columnMapping.json\", 'w') as json_file:\n",
    "    json.dump({key: child_instances_to_replace[key] for key in sorted(child_instances_to_replace)} , json_file, indent=4)\n",
    "\n",
    "with open(f\"C:/Users/vbohara/Desktop/Varun/Convtr/OutputQueries/{file_name}/{file_name}_tableMapping.json\", 'w') as json_file:\n",
    "    json.dump(table_mapping_dict, json_file, indent=4)\n",
    "\n",
    "with open(output_path, 'w') as file:\n",
    "    file.write(output_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
